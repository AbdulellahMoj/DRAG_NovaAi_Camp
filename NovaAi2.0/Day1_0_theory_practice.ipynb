{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/Header_NovaAi_Camp2.0.png\" alt=\"NovaAi Camp Header\" width=\"100%\">\n",
    "\n",
    "# ðŸŽ¯ Lab 1: Machine Learning Foundations\n",
    "## From Theory to Practice - Building Your First ML Models\n",
    "\n",
    "**Created by:** Abdulellah Mojalled  \n",
    "**Course:** NovaAI Camp 2.0 - Machine Learning Deep Dive  \n",
    "**Day 1:** ML Foundations\n",
    "\n",
    "---\n",
    "\n",
    "Machine Learning isn't magic - it's **mathematics** meets **data**! In this lab, you'll transform from theory to practice by building real ML models from scratch. We'll go slow, explain every step, and visualize everything so you truly understand what's happening under the hood.\n",
    "\n",
    "### ðŸ“š **What You'll Master Today:**\n",
    "\n",
    "âœ… **Part 1: Data Preparation** - Why \"Garbage In, Garbage Out\" is the golden rule  \n",
    "âœ… **Part 2: The Three Pillars** - Model, Loss Function, and Optimizer (the core of ALL ML)  \n",
    "âœ… **Part 3: Generalization** - The battle between Overfitting and Underfitting  \n",
    "âœ… **Part 4: Classification Metrics** - Why 99% accuracy can mean 0% value  \n",
    "âœ… **Part 5: Robust Evaluation** - Cross-validation and how to trust your models\n",
    "\n",
    "### ðŸŽ“ **Learning Approach:**\n",
    "\n",
    "This lab is designed for **beginners** who want to truly understand ML foundations:\n",
    "- ðŸ§© **Small Steps:** Each concept broken into bite-sized pieces\n",
    "- ðŸ“Š **Visual Learning:** Every concept visualized with plots and diagrams\n",
    "- ðŸ’» **Hands-On:** Run code immediately, see results instantly\n",
    "- ðŸŽ¯ **Real Examples:** House prices, disease detection, and practical scenarios\n",
    "\n",
    "### ðŸ”§ **Prerequisites:**\n",
    "\n",
    "Before starting, make sure you're comfortable with:\n",
    "- âœ“ Python basics (variables, functions, loops)\n",
    "- âœ“ NumPy arrays and basic operations\n",
    "- âœ“ Pandas DataFrames (we covered this in previous labs!)\n",
    "- âœ“ Matplotlib for plotting (we'll guide you through)\n",
    "\n",
    "\n",
    "\n",
    "**Ready? Let's dive into the world of Machine Learning! ðŸŒŠ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "## Why Your Model is Only as Good as Your Data\n",
    "\n",
    "Let me tell you something they don't mention in most ML tutorials: you're going to spend way more time cleaning data than building models. Some say 80% of the work is data preparation, 20% is the actual modeling. Today, you'll see exactly why.\n",
    "\n",
    "### The Stop Sign Problem\n",
    "\n",
    "Imagine you're training a self-driving car. You feed it thousands of images to learn from. But there's a problem - someone mislabeled the data. Stop signs are labeled as \"Green Light\". Traffic lights are labeled as \"Pedestrian Crossing\".\n",
    "\n",
    "What happens? Your model learns perfectly... learns the wrong thing. It will confidently run every stop sign it sees because that's what the data taught it.\n",
    "\n",
    "This is \"Garbage In, Garbage Out\" in action.\n",
    "\n",
    "### Why Models Fail (The Real Reasons)\n",
    "\n",
    "When a model performs poorly, there are usually two culprits:\n",
    "\n",
    "**Bad Data** (90% of the time)\n",
    "- Missing information (blank cells, null values)\n",
    "- Wrong labels or measurements\n",
    "- Outliers that don't make sense\n",
    "- Inconsistent formats\n",
    "\n",
    "**Bad Algorithm** (10% of the time)\n",
    "- Wrong model for the problem\n",
    "- Poor hyperparameters\n",
    "- Not enough training\n",
    "\n",
    "Notice something? Data quality problems are 9 times more common than algorithm problems. Yet most beginners obsess over which algorithm to use while ignoring their messy data.\n",
    "\n",
    "### What We'll Fix Today\n",
    "\n",
    "Real data is messy. Always. Whether it's from sensors, user input, or manual entry, you'll deal with:\n",
    "\n",
    "**Missing Values**\n",
    "- Someone forgot to fill in their age\n",
    "- A sensor failed and recorded nothing\n",
    "- Data got corrupted during transfer\n",
    "\n",
    "**Scale Problems**  \n",
    "- House size: 500-5000 square feet\n",
    "- Age: 1-100 years\n",
    "- Price: $50,000-$5,000,000\n",
    "\n",
    "Without fixing this, your model will think a house that's 10 years newer is basically the same, but 100 square feet smaller is a huge difference. The numbers overwhelm each other.\n",
    "\n",
    "**Text Data**\n",
    "- Models need numbers, not words\n",
    "- \"Yes\" and \"No\" need to become 1 and 0\n",
    "- Categories like \"Red\", \"Blue\", \"Green\" need numerical representation\n",
    "\n",
    "### Our Practice Dataset\n",
    "\n",
    "You're a data scientist at a real estate company. You get handed a spreadsheet with house data. It has:\n",
    "- `SquareFeet` - house size\n",
    "- `Age` - how old the building is (some missing!)\n",
    "- `Garage` - Yes or No (some missing!)\n",
    "- `Price` - what it sold for\n",
    "\n",
    "Your job: clean it up so a model can learn from it.\n",
    "\n",
    "Let's start by importing our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the basics - data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn tools for data preparation\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "print(\"Scikit-learn preprocessing tools loaded âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make our plots look good\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Visualization ready âœ“\")\n",
    "print(\"Random seed set to 42 (results will be consistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creating Our Messy Dataset\n",
    "\n",
    "Before we can clean data, we need some messy data to work with. Let's create a small house dataset with the kind of problems you'll see in the real world.\n",
    "\n",
    "We're making it small (10 houses) so you can actually see what's happening. In practice, you'd have thousands or millions of rows, but the problems are the same.\n",
    "\n",
    "**What we're creating:**\n",
    "- 10 houses with 4 pieces of information each\n",
    "- `SquareFeet` - complete data\n",
    "- `Age` - missing for 2 houses (realistic - old records get lost)\n",
    "- `Garage` - missing for 2 houses (someone didn't fill out the form)\n",
    "- `Price` - complete data (you always know what it sold for)\n",
    "\n",
    "The missing values will show up as `NaN` which stands for \"Not a Number\" - that's how Python represents \"no data here\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our house dataset with intentional missing values\n",
    "data = {\n",
    "    'SquareFeet': [1200, 1500, 1800, 2000, 2500, 1100, 1700, 2200, 1600, 1900],\n",
    "    'Age': [10, 5, np.nan, 20, 15, 30, np.nan, 8, 12, 25],      # Missing: rows 2 and 6\n",
    "    'Garage': ['Yes', 'Yes', 'No', np.nan, 'Yes', 'No', 'Yes', np.nan, 'No', 'Yes'],  # Missing: rows 3 and 7\n",
    "    'Price': [250000, 300000, 350000, 400000, 500000, 220000, 380000, 450000, 310000, 420000]\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(data)\n",
    "print(\"Dataset created. Let's see what we have:\\n\")\n",
    "print(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df_raw.isnull().sum())\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Age: 2 houses are missing age data\")\n",
    "print(\"- Garage: 2 houses are missing garage information\")\n",
    "print(\"- SquareFeet and Price: All data present\")\n",
    "print(\"\\nWhy this matters: Most ML algorithms will crash if they see NaN values\")\n",
    "print(\"We need to fix this before we can do anything else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize where the missing data is\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Create a heatmap showing missing values\n",
    "sns.heatmap(df_raw.isnull(), cmap='YlOrRd', cbar=True, yticklabels=True)\n",
    "plt.title('Missing Data Pattern (Red = Missing)', fontsize=13)\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Row Number')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"What you're seeing:\")\n",
    "print(\"- Each row is one house\")\n",
    "print(\"- Red cells = missing data\")\n",
    "print(\"- Blue cells = data is there\")\n",
    "print(\"- Rows 2 and 6 are missing Age\")\n",
    "print(\"- Rows 3 and 7 are missing Garage status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imputation - Filling the Gaps\n",
    "\n",
    "We've got missing data. Now what? We can't just leave those NaN values - our model will crash. We have a few options:\n",
    "\n",
    "**Option 1: Delete rows with missing data**\n",
    "- Simple, but wasteful\n",
    "- If you have 10,000 rows and 100 have missing data, deleting them is fine\n",
    "- If you have 100 rows and 50 have missing data, you just lost half your dataset\n",
    "\n",
    "**Option 2: Fill them in (Imputation)**\n",
    "- Keep all your data\n",
    "- Fill missing values with reasonable guesses\n",
    "- This is what we'll do\n",
    "\n",
    "### How to Choose What to Fill In\n",
    "\n",
    "The strategy depends on the type of data:\n",
    "\n",
    "**For Numbers (Age, Income, Square Feet):**\n",
    "- **Mean** (average) - good for normally distributed data\n",
    "- **Median** (middle value) - better if you have outliers\n",
    "- **Mode** (most common) - for discrete counts\n",
    "\n",
    "**For Categories (Yes/No, Red/Blue/Green):**\n",
    "- **Most Frequent** - whatever appears most often\n",
    "\n",
    "Our approach:\n",
    "- `Age` is a number â†’ use **mean** (average age)\n",
    "- `Garage` is Yes/No â†’ use **most frequent** (whichever appears more)\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what the Age column looks like\n",
    "print(\"Age column statistics (NaN values are automatically ignored):\")\n",
    "print(f\"Mean (average): {df_raw['Age'].mean():.1f} years\")\n",
    "print(f\"Median (middle value): {df_raw['Age'].median():.1f} years\")\n",
    "print(f\"Min: {df_raw['Age'].min():.0f} years\")\n",
    "print(f\"Max: {df_raw['Age'].max():.0f} years\")\n",
    "\n",
    "print(f\"\\nWe'll fill missing ages with the mean: {df_raw['Age'].mean():.1f} years\")\n",
    "print(\"This is a reasonable guess based on the other houses we know about.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing Age values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_raw['Age'] = imputer.fit_transform(df_raw[['Age']])\n",
    "\n",
    "print(\"Age column after filling missing values:\")\n",
    "print(df_raw['Age'])\n",
    "print(\"\\nNotice: Rows 2 and 6 now have values (the mean age) instead of NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the Garage column\n",
    "print(\"Garage column - how many of each:\")\n",
    "print(df_raw['Garage'].value_counts())\n",
    "\n",
    "print(f\"\\nMost common value: '{df_raw['Garage'].mode()[0]}'\")\n",
    "print(\"We'll fill missing Garage values with this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing Garage values with most frequent\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_raw['Garage'] = imputer.fit_transform(df_raw[['Garage']]).ravel()\n",
    "\n",
    "print(\"Complete dataset after filling all missing values:\\n\")\n",
    "print(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no missing values remain\n",
    "print(\"Final check for missing values:\")\n",
    "print(df_raw.isnull().sum())\n",
    "\n",
    "if df_raw.isnull().sum().sum() == 0:\n",
    "    print(\"\\nâœ“ Success! No more missing values.\")\n",
    "    print(\"Dataset is ready for the next step.\")\n",
    "else:\n",
    "    print(\"\\nâš  Still have missing values - something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Scaling - Making Numbers Comparable\n",
    "\n",
    "Here's a problem you might not see coming. Look at our data:\n",
    "- `SquareFeet` ranges from 1,100 to 2,500\n",
    "- `Age` ranges from 5 to 30\n",
    "\n",
    "Seems fine, right? Wrong. Many machine learning algorithms treat larger numbers as \"more important\". They calculate distances between data points. When you're computing distances with numbers on different scales, the bigger numbers dominate.\n",
    "\n",
    "### The Teacher Grading Problem\n",
    "\n",
    "Imagine a teacher grades students on two things:\n",
    "- Exam score: 0-100 points\n",
    "- Participation: 0-10 points\n",
    "\n",
    "Student A: 90 (exam) + 5 (participation) = 95 total\n",
    "Student B: 50 (exam) + 10 (participation) = 60 total\n",
    "\n",
    "The exam score drowns out participation. Student B could have perfect participation and it barely matters.\n",
    "\n",
    "This is what happens in our house data. A 100 square foot difference looks huge compared to a 10 year difference, even though both might be equally important.\n",
    "\n",
    "### The Solution: Standardization\n",
    "\n",
    "We transform all features to have:\n",
    "- **Mean = 0**\n",
    "- **Standard Deviation = 1**\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $x$ = original value\n",
    "- $\\mu$ = mean of all values\n",
    "- $\\sigma$ = standard deviation\n",
    "- $z$ = standardized value\n",
    "\n",
    "In plain English: subtract the average, divide by the spread. That's it.\n",
    "\n",
    "After standardization, all features are on the same scale. A house that's 1 standard deviation larger is directly comparable to a house that's 1 standard deviation newer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the problem first - look at the different scales\n",
    "print(\"BEFORE Scaling - see the different ranges:\\n\")\n",
    "print(df_raw[['SquareFeet', 'Age']].describe())\n",
    "\n",
    "print(f\"\\nSquareFeet ranges from {df_raw['SquareFeet'].min()} to {df_raw['SquareFeet'].max()}\")\n",
    "print(f\"Age ranges from {df_raw['Age'].min():.1f} to {df_raw['Age'].max():.1f}\")\n",
    "print(\"\\nSquareFeet values are about 100x larger than Age values.\")\n",
    "print(\"Many ML algorithms will think SquareFeet is way more important!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Make a copy so we keep the original\n",
    "df_scaled = df_raw.copy()\n",
    "\n",
    "# Standardize the numerical columns\n",
    "df_scaled[['SquareFeet', 'Age']] = scaler.fit_transform(df_raw[['SquareFeet', 'Age']])\n",
    "\n",
    "print(\"After scaling:\")\n",
    "print(df_scaled[['SquareFeet', 'Age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the statistics after scaling\n",
    "print(\"AFTER Scaling - both on same scale now:\\n\")\n",
    "print(df_scaled[['SquareFeet', 'Age']].describe())\n",
    "\n",
    "print(\"\\nWhat happened:\")\n",
    "print(\"- Both columns now have mean â‰ˆ 0\")\n",
    "print(\"- Both have standard deviation â‰ˆ 1\")\n",
    "print(\"- They're on the same scale\")\n",
    "print(\"\\nImportant:\")\n",
    "print(\"- Negative values are normal (they mean 'below average')\")\n",
    "print(\"- We didn't change relationships between points, just the scale\")\n",
    "print(\"- The original pattern is preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Encoding - Teaching Machines to Read Text\n",
    "\n",
    "Computers can't do math on words. They need numbers. Our `Garage` column has \"Yes\" and \"No\" - we need to convert these to something a model can work with.\n",
    "\n",
    "There are two main ways to do this:\n",
    "\n",
    "### 1.4.1 Label Encoding: Assign Numbers to Categories\n",
    "\n",
    "Simple approach: give each category a number.\n",
    "- Red â†’ 0\n",
    "- Blue â†’ 1\n",
    "- Green â†’ 2\n",
    "\n",
    "**The Problem:** The model might think there's an order. It might think Green (2) > Blue (1) > Red (0). Maybe it assumes 2 is \"twice as much\" as 1. For colors, that doesn't make sense.\n",
    "\n",
    "**When to use:** Only when there's a real order\n",
    "- T-shirt sizes: Small (0) < Medium (1) < Large (2) âœ“\n",
    "- Education: High School (0) < Bachelor (1) < Master (2) < PhD (3) âœ“  \n",
    "- Colors: Red (0), Blue (1), Green (2) âœ— - there's no order here!\n",
    "\n",
    "### 1.4.2 One-Hot Encoding: Create Binary Columns\n",
    "\n",
    "Instead of one column with multiple values, create one column per category:\n",
    "\n",
    "Original:\n",
    "```\n",
    "Garage\n",
    "------\n",
    "Yes\n",
    "No\n",
    "Yes\n",
    "```\n",
    "\n",
    "After One-Hot:\n",
    "```\n",
    "Garage_Yes\n",
    "----------\n",
    "1\n",
    "0\n",
    "1\n",
    "```\n",
    "\n",
    "With one-hot encoding, there's no implied order. \"Yes\" isn't \"greater than\" or \"less than\" \"No\" - they're just different.\n",
    "\n",
    "We'll use both methods so you can see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”§ Apply One-Hot Encoding to Garage Column\n",
    "\n",
    "print(\"ðŸ“‹ Current Garage column values:\")\n",
    "print(df_scaled['Garage'].unique())\n",
    "print(f\"\\n   We have 2 categories: {list(df_scaled['Garage'].unique())}\")\n",
    "\n",
    "# Create OneHotEncoder object\n",
    "# sparse_output=False: Return a regular array, not sparse matrix\n",
    "# drop='first': Drop the first category to avoid multicollinearity\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "# Apply encoding\n",
    "garage_encoded = encoder.fit_transform(df_scaled[['Garage']])\n",
    "\n",
    "print(f\"\\nâœ… Encoding complete!\")\n",
    "print(f\"   Original categories: {encoder.categories_[0]}\")\n",
    "print(f\"   Dropped category: '{encoder.categories_[0][0]}' (becomes the reference)\")\n",
    "print(f\"   Kept category: '{encoder.categories_[0][1]}'\")\n",
    "print(f\"\\n   Shape of encoded data: {garage_encoded.shape}\")\n",
    "print(f\"   (10 rows, 1 column for 'Garage_Yes')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ Create Final Dataset with Encoded Feature\n",
    "\n",
    "# Create a copy and add the encoded column\n",
    "df_final = df_scaled.copy()\n",
    "df_final['Garage_Yes'] = garage_encoded[:, 0]  # Add the new binary column\n",
    "\n",
    "# Drop the original Garage column (we don't need it anymore)\n",
    "df_final = df_final.drop('Garage', axis=1)\n",
    "\n",
    "print(\"ðŸ“‹ Final Cleaned Dataset:\")\n",
    "print(\"=\"*70)\n",
    "print(df_final)\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ’¡ Understanding the columns:\")\n",
    "print(\"   - SquareFeet: Standardized (mean=0, std=1)\")\n",
    "print(\"   - Age: Standardized (mean=0, std=1)\")\n",
    "print(\"   - Garage_Yes: Binary encoded (1=Yes, 0=No)\")\n",
    "print(\"   - Price: Original values (our target - we don't scale the target!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š Visualization: Before vs. After Scaling\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# LEFT: Raw data (before scaling)\n",
    "axes[0].scatter(data['SquareFeet'], data['Age'], c='red', s=120, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "axes[0].set_xlabel('SquareFeet (Original Scale)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Age (Original Scale)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('ðŸ”´ BEFORE Scaling\\n(Different Scales - Problem!)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.4, linestyle='--')\n",
    "axes[0].set_xlim(1000, 2600)\n",
    "axes[0].set_ylim(0, 35)\n",
    "\n",
    "# Add annotation showing scale difference\n",
    "axes[0].annotate('SquareFeet values: 1100-2500\\n(Much larger!)', \n",
    "                xy=(2000, 30), fontsize=10, color='darkred', \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# RIGHT: Scaled data (after standardization)\n",
    "axes[1].scatter(df_scaled['SquareFeet'], df_scaled['Age'], c='blue', s=120, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "axes[1].set_xlabel('SquareFeet (Standardized)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Age (Standardized)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('ðŸ”µ AFTER Scaling\\n(Same Scale - Fixed! âœ“)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.4, linestyle='--')\n",
    "axes[1].axhline(y=0, color='green', linestyle='--', linewidth=2, alpha=0.5, label='Mean = 0')\n",
    "axes[1].axvline(x=0, color='green', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "# Add annotation showing standardization\n",
    "axes[1].annotate('Both features now:\\nMeanâ‰ˆ0, Stdâ‰ˆ1', \n",
    "                xy=(0.5, 1.5), fontsize=10, color='darkblue', \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ¨ Key Takeaway:\")\n",
    "print(\"   - LEFT: Original data has wildly different scales\")\n",
    "print(\"   - RIGHT: Standardized data is centered at (0,0) with equal spread\")\n",
    "print(\"   - The PATTERN is preserved, just the scale changed!\")\n",
    "print(\"   - Now both features contribute equally to the model! âš–ï¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Three Pillars of Supervised Learning\n",
    "## Building Your First Regression Model\n",
    "\n",
    "### How Does a Machine Actually \"Learn\"?\n",
    "\n",
    "Think about teaching a kid to estimate house prices. You show them examples:\n",
    "- \"This 2,000 sq ft house sold for $300,000\"\n",
    "- \"This 1,500 sq ft house sold for $225,000\"\n",
    "\n",
    "After enough examples, they pick up the pattern: bigger houses cost more. They can now estimate prices for houses they've never seen.\n",
    "\n",
    "Machine learning works the same way. Instead of a kid, we have a mathematical function. Instead of intuition, we have three components working together.\n",
    "\n",
    "### The Three Components\n",
    "\n",
    "Every supervised learning algorithm - whether it's simple linear regression or a complex neural network - has these three pieces:\n",
    "\n",
    "**1. The Model (the learner)**\n",
    "\n",
    "This is the mathematical function that makes predictions. For linear regression, it's simple:\n",
    "\n",
    "$$\\text{Predicted Price} = w \\times \\text{SquareFeet} + b$$\n",
    "\n",
    "Where:\n",
    "- $w$ = weight (how much each square foot adds to price)\n",
    "- $b$ = bias (base price when square feet = 0)\n",
    "\n",
    "Think of $w$ and $b$ as knobs we need to adjust to make good predictions.\n",
    "\n",
    "**2. The Loss Function (the evaluator)**\n",
    "\n",
    "This measures how wrong our predictions are. For regression, we typically use Mean Squared Error (MSE):\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Translation: for each house, find the error (actual - predicted), square it, then average all the squared errors.\n",
    "\n",
    "Why square? Because errors can be positive or negative, and we don't want them canceling out. Also, squaring punishes big mistakes more than small ones.\n",
    "\n",
    "**3. The Optimizer (the adjuster)**\n",
    "\n",
    "This is the algorithm that adjusts those knobs ($w$ and $b$) to minimize the loss. Most commonly: Gradient Descent.\n",
    "\n",
    "Imagine you're blindfolded on a hillside and trying to get to the valley. You:\n",
    "- Feel the slope under your feet (that's the gradient)\n",
    "- Take a step downhill\n",
    "- Repeat until you can't go any lower\n",
    "\n",
    "That's gradient descent. The slope tells you which direction makes the loss smaller, and you keep adjusting until you can't improve anymore.\n",
    "\n",
    "### What We'll Build\n",
    "\n",
    "We're going to predict house prices from square footage. You'll see all three components in action:\n",
    "- Create a model\n",
    "- Train it (optimizer finds best $w$ and $b$)\n",
    "- Evaluate it (use loss function to measure performance)\n",
    "- See how well it predicts on data it's never seen\n",
    "\n",
    "Let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import what we need for regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"Regression tools loaded âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating Practice Data\n",
    "\n",
    "We'll generate fake house data where we control the relationship. This way, we can check if our model actually learns it.\n",
    "\n",
    "The secret relationship (that we'll hide from the model):\n",
    "```\n",
    "Price = 150 Ã— SquareFeet + random noise\n",
    "```\n",
    "\n",
    "The model only sees the square feet and prices. It has to discover that 150 multiplier on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic house data\n",
    "np.random.seed(42)\n",
    "\n",
    "# House sizes: 100 houses between 1000-3000 sq ft\n",
    "X = np.random.uniform(1000, 3000, 100).reshape(-1, 1)\n",
    "\n",
    "# Prices: $150 per sq ft, plus noise\n",
    "true_weight = 150\n",
    "noise = np.random.normal(0, 30000, 100)  # Random variation ~$30k\n",
    "y = true_weight * X.ravel() + noise\n",
    "\n",
    "print(f\"Dataset created: {len(X)} houses\")\n",
    "print(f\"Size range: {X.min():.0f} to {X.max():.0f} sq ft\")\n",
    "print(f\"Price range: ${y.min():,.0f} to ${y.max():,.0f}\")\n",
    "print(f\"\\nHidden relationship: Price = {true_weight} Ã— SquareFeet + noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.scatter(X, y, alpha=0.6, edgecolors='black', s=70, color='steelblue')\n",
    "plt.xlabel('Square Feet', fontsize=12)\n",
    "plt.ylabel('Price ($)', fontsize=12)\n",
    "plt.title('House Prices vs Size', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"What you're seeing:\")\n",
    "print(\"- Each dot is one house\")\n",
    "print(\"- Clear trend: bigger = more expensive\")\n",
    "print(\"- But not a perfect line (that's the noise)\")\n",
    "print(\"- Our job: find the best line through these points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train/Test Split\n",
    "\n",
    "Here's the critical part that beginners often get wrong: you CANNOT test your model on the same data it learned from.\n",
    "\n",
    "Think of it like studying for an exam:\n",
    "- **Training set** = practice problems you study with\n",
    "- **Test set** = the actual exam (different questions!)\n",
    "\n",
    "If you test on your practice problems, you'll get a perfect score - but that doesn't mean you actually understand the material. You might have just memorized the answers.\n",
    "\n",
    "Same with machine learning. If we test on training data, we can't tell if the model learned the pattern or just memorized the specific examples.\n",
    "\n",
    "We'll use an 80/20 split: train on 80% of the data, test on the other 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} houses (80%)\")\n",
    "print(f\"Test set: {len(X_test)} houses (20%)\")\n",
    "print(\"\\nThe model will learn from the training set.\")\n",
    "print(\"We'll evaluate it on the test set (data it's never seen).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training the Model\n",
    "\n",
    "Now we create and train our linear regression model. In scikit-learn, this is deceptively simple - just two lines of code. But there's a lot happening behind the scenes.\n",
    "\n",
    "When we call `.fit()`, the optimizer:\n",
    "1. Starts with random values for $w$ and $b$\n",
    "2. Makes predictions with those random values (they'll be terrible)\n",
    "3. Calculates how wrong those predictions are (the loss)\n",
    "4. Adjusts $w$ and $b$ to reduce the loss\n",
    "5. Repeats until it can't improve anymore\n",
    "\n",
    "For linear regression, sklearn uses a mathematical shortcut called the \"Normal Equation\" instead of iterative gradient descent. It's faster for small datasets like ours.\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained!\")\n",
    "print(\"\\nWhat it learned:\")\n",
    "print(f\"Weight (w): ${model.coef_[0]:.2f} per square foot\")\n",
    "print(f\"Bias (b): ${model.intercept_:,.2f}\")\n",
    "\n",
    "print(f\"\\nModel's equation:\")\n",
    "print(f\"Price = {model.coef_[0]:.2f} Ã— SquareFeet + {model.intercept_:,.2f}\")\n",
    "\n",
    "print(f\"\\nActual relationship:\")\n",
    "print(f\"Price = {true_weight} Ã— SquareFeet + 0\")\n",
    "\n",
    "print(f\"\\nDid it learn correctly? Weight should be close to {true_weight}.\")\n",
    "print(f\"Learned weight: {model.coef_[0]:.0f} âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated for test houses.\\n\")\n",
    "print(\"First 5 comparisons:\")\n",
    "print(f\"{'Size (sqft)':<12} {'Predicted':<15} {'Actual':<15} {'Error':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(5):\n",
    "    sqft = X_test[i][0]\n",
    "    pred = y_pred[i]\n",
    "    actual = y_test[i]\n",
    "    error = abs(pred - actual)\n",
    "    print(f\"{sqft:<12.0f} ${pred:<14,.0f} ${actual:<14,.0f} ${error:<9,.0f}\")\n",
    "\n",
    "print(\"\\nPredictions are close but not perfect (expected due to noise).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Measuring Performance\n",
    "\n",
    "We have predictions. But how good are they? We need metrics to measure model quality.\n",
    "\n",
    "**Three common metrics:**\n",
    "\n",
    "**Mean Squared Error (MSE)** \n",
    "- Average of squared errors\n",
    "- Punishes big mistakes heavily (they get squared!)\n",
    "- Hard to interpret (units are dollarsÂ²)\n",
    "- Lower is better\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "- Average of absolute errors  \n",
    "- Same units as our target (dollars)\n",
    "- Easy to interpret: \"on average, we're off by $X\"\n",
    "- Lower is better\n",
    "\n",
    "**RÂ² Score**\n",
    "- \"Percentage of variance explained\"\n",
    "- Range: 0 to 1 (1 = perfect)\n",
    "- Tells you how much better your model is than just guessing the average\n",
    "- Higher is better\n",
    "\n",
    "Let's calculate all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Performance on test set:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"MSE:  ${mse:,.0f}\")\n",
    "print(f\"MAE:  ${mae:,.0f}\")\n",
    "print(f\"RÂ²:   {r2:.3f} ({r2*100:.1f}%)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"MAE = ${mae:,.0f}\")\n",
    "print(f\"  â†’ On average, predictions are off by ${mae:,.0f}\")\n",
    "print(f\"  â†’ For houses averaging ${y_test.mean():,.0f}, that's decent\")\n",
    "\n",
    "print(f\"\\nRÂ² = {r2:.3f}\")\n",
    "print(f\"  â†’ Model explains {r2*100:.1f}% of price variation\")\n",
    "print(f\"  â†’ The remaining {(1-r2)*100:.1f}% is unexplained\")\n",
    "\n",
    "if r2 > 0.85:\n",
    "    print(\"  â†’ Excellent performance!\")\n",
    "elif r2 > 0.70:\n",
    "    print(\"  â†’ Good performance\")\n",
    "else:\n",
    "    print(\"  â†’ Room for improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Visualizing the Fit\n",
    "\n",
    "Numbers are good, but seeing is better. Let's plot our model's predictions against the actual data.\n",
    "\n",
    "If the model is working well, the data points should cluster around the prediction line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test data vs model predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Actual test data\n",
    "plt.scatter(X_test, y_test, alpha=0.6, s=80, \n",
    "           label='Actual prices', color='steelblue')\n",
    "\n",
    "# Model's prediction line\n",
    "X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_range = model.predict(X_range)\n",
    "plt.plot(X_range, y_range, color='red', linewidth=2.5,\n",
    "        label=f'Model: y = {model.coef_[0]:.0f}x + {model.intercept_:.0f}')\n",
    "\n",
    "plt.xlabel('Square Feet', fontsize=12)\n",
    "plt.ylabel('Price ($)', fontsize=12)\n",
    "plt.title('Model Fit on Test Data', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add stats\n",
    "plt.text(0.98, 0.05, f'RÂ² = {r2:.3f}\\nMAE = ${mae:,.0f}', \n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=11, ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Blue dots = actual test data\")\n",
    "print(\"Red line = model predictions\")\n",
    "print(\"Dots close to line = good predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at prediction errors (residuals)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Left: Residual plot\n",
    "ax1.scatter(y_pred, residuals, alpha=0.6, s=70, color='coral')\n",
    "ax1.axhline(y=0, color='darkred', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Predicted Price ($)', fontsize=11)\n",
    "ax1.set_ylabel('Error (Actual - Predicted)', fontsize=11)\n",
    "ax1.set_title('Residual Plot', fontsize=13)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Error distribution\n",
    "ax2.hist(residuals, bins=15, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "ax2.axvline(x=0, color='darkred', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Error', fontsize=11)\n",
    "ax2.set_ylabel('Frequency', fontsize=11)\n",
    "ax2.set_title('Error Distribution', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"What to look for:\")\n",
    "print(\"\\nLeft plot:\")\n",
    "print(\"- Points randomly scattered around zero = good\")\n",
    "print(\"- Pattern visible = model missing something\")\n",
    "\n",
    "print(\"\\nRight plot:\")\n",
    "print(\"- Bell curve centered at zero = ideal\")\n",
    "print(\"- Shows errors are unbiased and normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Part 2 Summary\n",
    "\n",
    "You just built your first complete machine learning model. Here's what happened:\n",
    "\n",
    "**Component 1: The Model**\n",
    "- Used Linear Regression: $\\hat{y} = wx + b$\n",
    "- The `.fit()` method found optimal $w$ and $b$\n",
    "- Model discovered the relationship: Price â‰ˆ $150 per square foot\n",
    "\n",
    "**Component 2: Loss Function**\n",
    "- Used MSE to measure prediction quality during training\n",
    "- Also calculated MAE (easier to interpret) and RÂ² (percentage explained)\n",
    "- Lower loss = better model\n",
    "\n",
    "**Component 3: Optimizer**  \n",
    "- Hidden inside `.fit()` - it used the Normal Equation\n",
    "- Automatically found the $w$ and $b$ that minimize MSE\n",
    "- No manual tuning needed\n",
    "\n",
    "**Key Lessons:**\n",
    "\n",
    "1. **Train/test split is essential** - Never evaluate on training data\n",
    "2. **Multiple metrics tell the full story** - MSE, MAE, and RÂ² each reveal something different\n",
    "3. **Visualization matters** - A plot can show problems that metrics miss\n",
    "4. **These three components appear in ALL supervised learning** - Neural networks, random forests, SVMs... they all have a model, loss function, and optimizer\n",
    "\n",
    "Next up: the biggest challenge in machine learning - getting your model to work on new data it's never seen. This is the generalization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generalization - The Real Test\n",
    "## Can Your Model Handle the Unexpected?\n",
    "\n",
    "You trained a model and it works great on your training data. Accuracy: 99.9%. You celebrate, deploy it to production, and... it crashes and burns. What happened?\n",
    "\n",
    "**You fell into the classic trap: your model memorized the training data instead of learning the pattern.**\n",
    "\n",
    "This is the generalization problem, and it's the reason most ML projects fail in the real world.\n",
    "\n",
    "### The Exam Analogy\n",
    "\n",
    "Remember studying for exams in school? Two approaches:\n",
    "\n",
    "**Student A** memorizes all the practice problems, word for word. Sees the exact same questions on the exam? Perfect score. Sees slightly different questions? Fails completely.\n",
    "\n",
    "**Student B** understands the underlying concepts. Might not remember every practice problem perfectly, but can solve new variations they've never seen.\n",
    "\n",
    "Machine learning is the same. We don't want a model that memorizes - we want one that understands.\n",
    "\n",
    "### The Goldilocks Problem\n",
    "\n",
    "When you build a model, you're trying to find the sweet spot:\n",
    "\n",
    "**Too Simple (Underfitting)**\n",
    "- Can't even capture the basic pattern\n",
    "- Like using a straight line to model a circle\n",
    "- Fails on both training AND test data\n",
    "\n",
    "**Too Complex (Overfitting)**\n",
    "- Memorizes every detail, including noise and errors\n",
    "- Like memorizing typos in a textbook\n",
    "- Perfect on training data, terrible on anything new\n",
    "\n",
    "**Just Right**\n",
    "- Captures the real pattern\n",
    "- Ignores random noise\n",
    "- Works on both training and new data\n",
    "\n",
    "Today we'll see this in action with a simple experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 Setting Up the Experiment\n",
    "\n",
    "We're going to create data with a known pattern - a wave. Then we'll train three models with different levels of complexity and see which one actually learns the pattern.\n",
    "\n",
    "The true pattern is a cosine wave: `y = cos(x)`. But we'll add some noise to make it realistic - real data is never perfect.\n",
    "\n",
    "Why a wave? Because it has curves. A simple straight line can't capture it (too simple), but an overly complex model might try to hit every noisy point instead of following the smooth wave (too complex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import polynomial tools\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "print(\"Polynomial regression tools loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "np.random.seed(42)\n",
    "\n",
    "X_nonlinear = np.linspace(0, 10, 50).reshape(-1, 1)  # 50 points from 0 to 10\n",
    "y_true = np.cos(X_nonlinear.ravel())  # True pattern: cosine wave\n",
    "noise = np.random.normal(0, 0.2, len(X_nonlinear))  # Add noise\n",
    "y_nonlinear = y_true + noise\n",
    "\n",
    "print(f\"Created {len(X_nonlinear)} data points\")\n",
    "print(f\"True function: y = cos(x)\")\n",
    "print(f\"Added noise with standard deviation = 0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train_nl, X_test_nl, y_train_nl, y_test_nl = train_test_split(\n",
    "    X_nonlinear, y_nonlinear, \n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_nl)} points (70%)\")\n",
    "print(f\"Test set: {len(X_test_nl)} points (30%)\")\n",
    "print(\"\\nThe model will learn from training, we'll evaluate on test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(X_train_nl, y_train_nl, alpha=0.7, s=80, \n",
    "           label='Training data', color='steelblue')\n",
    "plt.scatter(X_test_nl, y_test_nl, alpha=0.7, s=80, \n",
    "           label='Test data', color='coral', marker='s')\n",
    "plt.plot(X_nonlinear, y_true, color='green', linewidth=2.5, \n",
    "        label='True pattern: y = cos(x)', linestyle='--')\n",
    "\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Our Data: Noisy Cosine Wave', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Blue dots: training data (model sees these)\")\n",
    "print(\"Orange squares: test data (model has never seen these)\")\n",
    "print(\"Green line: the true pattern we want to learn\")\n",
    "print(\"\\nNotice the scatter around the line - that's noise. Real data always has it.\")\n",
    "print(\"Goal: learn the green curve, ignore the noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Three Different Models\n",
    "\n",
    "We'll train three polynomial regression models with different complexities:\n",
    "\n",
    "**Model 1: Degree 1 (Just a straight line)**\n",
    "- Equation: $y = w_1x + b$\n",
    "- Prediction: Too simple - can't capture the curve (underfit)\n",
    "\n",
    "**Model 2: Degree 5 (Moderate curve)**\n",
    "- Equation: $y = w_5x^5 + w_4x^4 + w_3x^3 + w_2x^2 + w_1x + b$\n",
    "- Prediction: Might be just right\n",
    "\n",
    "**Model 3: Degree 15 (Very wiggly curve)**\n",
    "- Equation: $y = w_{15}x^{15} + ... + w_1x + b$\n",
    "- Prediction: Too flexible - will memorize noise (overfit)\n",
    "\n",
    "Let's train all three and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 1: Degree 1 (linear)\n",
    "model_deg1 = make_pipeline(PolynomialFeatures(degree=1), LinearRegression())\n",
    "model_deg1.fit(X_train_nl, y_train_nl)\n",
    "\n",
    "print(\"Model 1 trained: Degree 1 (straight line)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 2: Degree 5 (moderate)\n",
    "model_deg5 = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())\n",
    "model_deg5.fit(X_train_nl, y_train_nl)\n",
    "\n",
    "print(\"Model 2 trained: Degree 5 (moderate polynomial)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model 3: Degree 15 (very high)\n",
    "model_deg15 = make_pipeline(PolynomialFeatures(degree=15), LinearRegression())\n",
    "model_deg15.fit(X_train_nl, y_train_nl)\n",
    "\n",
    "print(\"Model 3 trained: Degree 15 (high polynomial)\")\n",
    "print(\"\\nAll three models ready. Let's see how they compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Comparing the Three Models\n",
    "\n",
    "Now let's visualize what each model learned. We'll plot all three predictions on the same graph so you can see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for plotting\n",
    "X_range = np.linspace(0, 10, 200).reshape(-1, 1)  # Smooth curve for visualization\n",
    "\n",
    "y_pred_deg1 = model_deg1.predict(X_range)\n",
    "y_pred_deg5 = model_deg5.predict(X_range)\n",
    "y_pred_deg15 = model_deg15.predict(X_range)\n",
    "\n",
    "print(\"Predictions generated for all three models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three models\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "plt.scatter(X_train_nl, y_train_nl, alpha=0.7, s=90, \n",
    "           label='Training data', color='steelblue', zorder=5)\n",
    "\n",
    "plt.plot(X_nonlinear, y_true, color='green', linewidth=3, \n",
    "        label='True function: y = cos(x)', linestyle='--', zorder=4)\n",
    "\n",
    "plt.plot(X_range, y_pred_deg1, color='red', linewidth=2.5, \n",
    "        label='Degree 1 (Underfit)', linestyle='-')\n",
    "plt.plot(X_range, y_pred_deg5, color='blue', linewidth=2.5, \n",
    "        label='Degree 5 (Good fit)', linestyle='-')\n",
    "plt.plot(X_range, y_pred_deg15, color='purple', linewidth=2.5, \n",
    "        label='Degree 15 (Overfit)', linestyle='-')\n",
    "\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Underfitting vs Good Fit vs Overfitting', fontsize=14)\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Plot Shows\n",
    "\n",
    "**Red line (Degree 1)** - Just a straight line that can't capture the wave. Too simple, misses the pattern completely. This is underfitting.\n",
    "\n",
    "**Blue line (Degree 5)** - Smooth curve following the true green pattern. Doesn't try to hit every training point. Learned the signal, ignored the noise. This is a good fit.\n",
    "\n",
    "**Purple line (Degree 15)** - Wild, wiggly curve trying to connect every training point. Strays far from the true pattern in places. Memorized the noise instead of learning the wave. This is overfitting.\n",
    "\n",
    "The green dashed line is what we're trying to learn. Degree 5 (blue) comes closest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Measuring the Generalization Gap\n",
    "\n",
    "Pictures are great, but we need numbers. Let's calculate training error and test error for each model.\n",
    "\n",
    "The key metric: **Generalization Gap = Test Error - Training Error**\n",
    "\n",
    "A large gap means the model is overfitting - it performs well on training data but fails on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors for all three models\n",
    "train_mse_deg1 = mean_squared_error(y_train_nl, model_deg1.predict(X_train_nl))\n",
    "test_mse_deg1 = mean_squared_error(y_test_nl, model_deg1.predict(X_test_nl))\n",
    "gap1 = test_mse_deg1 - train_mse_deg1\n",
    "\n",
    "train_mse_deg5 = mean_squared_error(y_train_nl, model_deg5.predict(X_train_nl))\n",
    "test_mse_deg5 = mean_squared_error(y_test_nl, model_deg5.predict(X_test_nl))\n",
    "gap5 = test_mse_deg5 - train_mse_deg5\n",
    "\n",
    "train_mse_deg15 = mean_squared_error(y_train_nl, model_deg15.predict(X_train_nl))\n",
    "test_mse_deg15 = mean_squared_error(y_test_nl, model_deg15.predict(X_test_nl))\n",
    "gap15 = test_mse_deg15 - train_mse_deg15\n",
    "\n",
    "print(\"Performance Comparison\\n\")\n",
    "print(f\"Degree 1 (Underfit):\")\n",
    "print(f\"  Training MSE: {train_mse_deg1:.4f}\")\n",
    "print(f\"  Test MSE: {test_mse_deg1:.4f}\")\n",
    "print(f\"  Gap: {gap1:.4f}\")\n",
    "print(f\"  â†’ Both errors high - model can't learn the pattern\")\n",
    "\n",
    "print(f\"\\nDegree 5 (Good Fit):\")\n",
    "print(f\"  Training MSE: {train_mse_deg5:.4f}\")\n",
    "print(f\"  Test MSE: {test_mse_deg5:.4f}\")\n",
    "print(f\"  Gap: {gap5:.4f}\")\n",
    "print(f\"  â†’ Low errors, small gap - learned the pattern!\")\n",
    "\n",
    "print(f\"\\nDegree 15 (Overfit):\")\n",
    "print(f\"  Training MSE: {train_mse_deg15:.4f}\")\n",
    "print(f\"  Test MSE: {test_mse_deg15:.4f}\")\n",
    "print(f\"  Gap: {gap15:.4f}\")\n",
    "print(f\"  â†’ Training perfect, test terrible, HUGE gap - memorized noise!\")\n",
    "\n",
    "print(f\"\\nWinner: Degree 5\")\n",
    "print(f\"Lowest test error AND smallest generalization gap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the generalization gap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = ['Degree 1\\n(Underfit)', 'Degree 5\\n(Good Fit)', 'Degree 15\\n(Overfit)']\n",
    "train_errors = [train_mse_deg1, train_mse_deg5, train_mse_deg15]\n",
    "test_errors = [test_mse_deg1, test_mse_deg5, test_mse_deg15]\n",
    "gaps = [gap1, gap5, gap15]\n",
    "\n",
    "# Left: Training vs Test Error\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x_pos - width/2, train_errors, width, label='Training MSE', alpha=0.8)\n",
    "ax1.bar(x_pos + width/2, test_errors, width, label='Test MSE', alpha=0.8)\n",
    "ax1.set_xlabel('Model Complexity', fontsize=11)\n",
    "ax1.set_ylabel('Mean Squared Error', fontsize=11)\n",
    "ax1.set_title('Training vs Test Error', fontsize=13)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(models)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Generalization Gap\n",
    "colors = ['red', 'green', 'red']\n",
    "ax2.bar(models, gaps, alpha=0.8, color=colors)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('Model Complexity', fontsize=11)\n",
    "ax2.set_ylabel('Generalization Gap\\n(Test MSE - Train MSE)', fontsize=11)\n",
    "ax2.set_title('The Generalization Gap', fontsize=13)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left chart: Degree 5 has low error on both train and test\")\n",
    "print(\"Right chart: Degree 5 has the smallest gap - best generalization!\")\n",
    "print(\"\\nDegree 15 has near-zero training error but huge test error - classic overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Part 3 Summary\n",
    "\n",
    "You just experienced the fundamental challenge of machine learning: the bias-variance tradeoff.\n",
    "\n",
    "**Underfitting (High Bias)**\n",
    "- Model too simple to capture the pattern\n",
    "- High training error, high test error\n",
    "- Example: straight line for curved data\n",
    "- Fix: increase model complexity, add more features, train longer\n",
    "\n",
    "**Overfitting (High Variance)**\n",
    "- Model memorizes training data, including noise\n",
    "- Very low training error, high test error, large gap\n",
    "- Example: wiggly curve connecting every training point\n",
    "- Fix: simplify model, get more training data, use regularization\n",
    "\n",
    "**Good Fit (The Sweet Spot)**\n",
    "- Model captures signal, ignores noise\n",
    "- Low training error, low test error, small gap\n",
    "- Example: smooth curve following the true pattern\n",
    "- This is what we're after\n",
    "\n",
    "### The Generalization Gap\n",
    "\n",
    "$$\\text{Generalization Gap} = \\text{Test Error} - \\text{Training Error}$$\n",
    "\n",
    "This is your warning sign. A large gap means overfitting - your model won't work on real data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Training error alone is meaningless.** A model with perfect training accuracy might be useless in production.\n",
    "\n",
    "2. **Test performance is what matters.** Can your model handle data it's never seen?\n",
    "\n",
    "3. **Complexity is a double-edged sword.** Too simple fails, too complex also fails.\n",
    "\n",
    "4. **The sweet spot varies by dataset.** There's no universal \"best\" complexity - you have to experiment.\n",
    "\n",
    "5. **Always visualize when possible.** The Degree 15 model looked fine in the error numbers at first, but the plot revealed the wild oscillations.\n",
    "\n",
    "Next: we'll tackle classification and learn why 99% accuracy can be completely worthless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "print(\"Classification libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Metrics - When 99% Accuracy Is Worthless\n",
    "## The Accuracy Paradox\n",
    "\n",
    "Your model achieves 99% accuracy. Time to celebrate, right?\n",
    "\n",
    "Not so fast.\n",
    "\n",
    "Imagine you're building a medical AI to detect a rare disease that affects 1% of the population. You deploy your \"99% accurate\" model to production. Patients trust it with their lives.\n",
    "\n",
    "Here's the twist: your model simply predicts everyone is healthy. Never flags anyone as sick. Ever.\n",
    "\n",
    "**Accuracy: 99%** (because 99% of people really are healthy)  \n",
    "**Sick patients detected: 0%** (you caught nobody)\n",
    "\n",
    "That's the accuracy paradox. In imbalanced datasets, accuracy is useless - even dangerous.\n",
    "\n",
    "### The Real Question\n",
    "\n",
    "When you have imbalanced classes (like 95% healthy, 5% sick), accuracy doesn't tell you:\n",
    "- Did we catch the sick patients?\n",
    "- How many false alarms did we trigger?\n",
    "- Can we trust a \"sick\" prediction?\n",
    "\n",
    "Today we'll learn the metrics that actually matter: **Precision**, **Recall**, and **F1-Score**. These tell you what's really happening with your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 The Imbalanced Dataset Problem\n",
    "\n",
    "Let's create a realistic scenario: a hospital wants to predict if patients have a disease based on medical tests. The problem? Only 5% of patients actually have the disease.\n",
    "\n",
    "This mirrors real-world situations:\n",
    "- Fraud detection (most transactions are legitimate)\n",
    "- Cancer screening (most patients are healthy)\n",
    "- Manufacturing defects (most products are fine)\n",
    "- Spam detection (most emails aren't spam)\n",
    "\n",
    "We'll generate fake medical data with:\n",
    "- 1,000 patients\n",
    "- 20 measurements per patient (blood pressure, cholesterol, etc.)\n",
    "- 95% healthy, 5% sick\n",
    "\n",
    "The challenge: can our model find the 5% who need treatment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate imbalanced dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.95, 0.05],  # 95% healthy, 5% sick\n",
    "    flip_y=0.02,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset created: {len(y)} patients\")\n",
    "print(f\"Features: {X.shape[1]} measurements per patient\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Healthy (class 0): {sum(y == 0)} patients ({sum(y == 0)/len(y)*100:.1f}%)\")\n",
    "print(f\"  Sick (class 1): {sum(y == 1)} patients ({sum(y == 1)/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nThis is highly imbalanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class imbalance\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "bars = plt.bar(['Healthy (0)', 'Sick (1)'], counts, \n",
    "               color=['#2ecc71', '#e74c3c'], edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)} ({height/len(y)*100:.1f}%)',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.ylabel('Number of Patients', fontsize=12)\n",
    "plt.title('Class Distribution: The Imbalance Problem', fontsize=14)\n",
    "plt.ylim(0, max(counts) * 1.15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"A 'dumb' model that always predicts 'healthy' would get 95% accuracy!\")\n",
    "print(\"But it would miss every single sick patient.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training a Classifier\n",
    "\n",
    "We'll use Logistic Regression (despite the name, it's for classification, not regression).\n",
    "\n",
    "**How it works:** It calculates the probability a patient is sick based on their measurements. If the probability is over 50%, it predicts sick; otherwise, healthy.\n",
    "\n",
    "The formula: $$P(\\text{sick}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)}}$$\n",
    "\n",
    "This outputs a probability between 0 and 1.\n",
    "\n",
    "Before training, we need to split our data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data with stratification to preserve class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(y_train)} patients\")\n",
    "print(f\"  Healthy: {sum(y_train == 0)} ({sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Sick: {sum(y_train == 1)} ({sum(y_train == 1)/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTest set: {len(y_test)} patients\")\n",
    "print(f\"  Healthy: {sum(y_test == 0)} ({sum(y_test == 0)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Sick: {sum(y_test == 1)} ({sum(y_test == 1)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nStratification ensures both sets have the same 95/5 ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Model trained on 750 patients\")\n",
    "print(f\"Predictions made on {len(y_test)} test patients\")\n",
    "print(f\"\\nPrediction breakdown:\")\n",
    "print(f\"  Predicted healthy: {sum(y_pred == 0)}\")\n",
    "print(f\"  Predicted sick: {sum(y_pred == 1)}\")\n",
    "print(f\"\\nNow let's see if these predictions are correct...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Understanding the Confusion Matrix\n",
    "\n",
    "The confusion matrix is a table that shows all possible outcomes. It compares what the model predicted versus what was actually true.\n",
    "\n",
    "It has four cells:\n",
    "- **True Negative (TN)**: Predicted healthy, actually healthy (correct!)\n",
    "- **False Positive (FP)**: Predicted sick, actually healthy (false alarm)\n",
    "- **False Negative (FN)**: Predicted healthy, actually sick (dangerous miss!)\n",
    "- **True Positive (TP)**: Predicted sick, actually sick (correct!)\n",
    "\n",
    "Let's calculate it for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "TN = cm[0, 0]  # True Negatives\n",
    "FP = cm[0, 1]  # False Positives\n",
    "FN = cm[1, 0]  # False Negatives\n",
    "TP = cm[1, 1]  # True Positives\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nBreakdown:\")\n",
    "print(f\"  True Negatives (TN): {TN} - Correctly predicted healthy\")\n",
    "print(f\"  True Positives (TP): {TP} - Correctly predicted sick\")\n",
    "print(f\"  False Positives (FP): {FP} - Healthy but predicted sick (false alarm)\")\n",
    "print(f\"  False Negatives (FN): {FN} - Sick but predicted healthy (DANGEROUS!)\")\n",
    "print(f\"\\nTotal correct: {TN + TP}\")\n",
    "print(f\"Total errors: {FP + FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                display_labels=['Healthy (0)', 'Sick (1)'])\n",
    "disp.plot(cmap='Blues', ax=ax, values_format='d', colorbar=False)\n",
    "\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Actual Class', fontsize=12)\n",
    "ax.set_ylabel('Predicted Class', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Darker blue = higher count\")\n",
    "print(\"We want dark blue on the diagonal (correct predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Precision, Recall, and F1-Score\n",
    "\n",
    "Now we'll calculate the metrics that actually matter for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Metric Results:\\n\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.1f}%)\")\n",
    "print(f\"Recall: {recall:.4f} ({recall*100:.1f}%)\")\n",
    "print(f\"F1-Score: {f1:.4f} ({f1*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Each Metric Means\n",
    "\n",
    "**Accuracy** = $\\frac{TP + TN}{TP + TN + FP + FN}$ = $\\frac{\\text{Correct}}{\\text{Total}}$\n",
    "\n",
    "Overall correctness. Looks at all predictions. **Misleading for imbalanced data** - a model predicting everyone is healthy gets 95% accuracy but is useless.\n",
    "\n",
    "**Precision** = $\\frac{TP}{TP + FP}$ = $\\frac{\\text{True Positives}}{\\text{All Positive Predictions}}$\n",
    "\n",
    "Of all patients we predicted sick, how many were actually sick? Answers: \"Can I trust a positive prediction?\" High precision = few false alarms.\n",
    "\n",
    "**Recall** (Sensitivity) = $\\frac{TP}{TP + FN}$ = $\\frac{\\text{True Positives}}{\\text{All Actually Sick}}$\n",
    "\n",
    "Of all actually sick patients, how many did we catch? Answers: \"Are we missing sick patients?\" High recall = catching the sick patients. **Most important for medical diagnosis.**\n",
    "\n",
    "**F1-Score** = $2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "Harmonic mean of precision and recall. Balances both concerns. Good single metric for imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Metric comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [accuracy, precision, recall, f1]\n",
    "colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "bars = ax1.barh(metrics, values, color=colors, edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    ax1.text(val + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "             f'{val:.3f}', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax1.set_xlim(0, 1.1)\n",
    "ax1.set_xlabel('Score', fontsize=12)\n",
    "ax1.set_title('Metric Comparison', fontsize=13)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Right: Error breakdown\n",
    "error_types = ['False\\nPositives', 'False\\nNegatives']\n",
    "error_counts = [FP, FN]\n",
    "error_colors = ['#e67e22', '#c0392b']\n",
    "\n",
    "bars2 = ax2.bar(error_types, error_counts, color=error_colors, \n",
    "                edgecolor='black', linewidth=2)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}', ha='center', va='bottom', \n",
    "             fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Error Breakdown', fontsize=13)\n",
    "ax2.set_ylim(0, max(error_counts) * 1.2 if max(error_counts) > 0 else 1)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey insight:\")\n",
    "print(f\"Accuracy is {accuracy:.3f} - seems good!\")\n",
    "print(f\"But we're missing {FN} sick patients (False Negatives).\")\n",
    "print(f\"For medical diagnosis, recall is most important.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report summary\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=['Healthy (0)', 'Sick (1)']))\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nHow to read this:\")\n",
    "print(\"- Precision: Of predicted X, how many were correct?\")\n",
    "print(\"- Recall: Of actual X, how many did we catch?\")\n",
    "print(\"- F1-Score: Balance between precision and recall\")\n",
    "print(\"- Support: Number of actual samples in each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Part 4 Summary\n",
    "\n",
    "You just learned why accuracy can be dangerously misleading and what metrics to use instead.\n",
    "\n",
    "### The Accuracy Trap\n",
    "\n",
    "For imbalanced data (95% healthy, 5% sick), accuracy is worthless. A model that always predicts \"healthy\" gets 95% accuracy but catches zero sick patients.\n",
    "\n",
    "Formula: $$\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}}$$\n",
    "\n",
    "It doesn't distinguish between the classes, so it hides the real problem.\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "This 2x2 table shows everything:\n",
    "\n",
    "```\n",
    "           Actually Healthy    Actually Sick\n",
    "Predicted Healthy:    TN              FN\n",
    "Predicted Sick:       FP              TP\n",
    "```\n",
    "\n",
    "- **TN (True Negative)**: Correctly said healthy - good\n",
    "- **TP (True Positive)**: Correctly said sick - good\n",
    "- **FP (False Positive)**: False alarm - annoying but not deadly\n",
    "- **FN (False Negative)**: Missed a sick patient - dangerous!\n",
    "\n",
    "### The Real Metrics\n",
    "\n",
    "**Precision** = TP / (TP + FP)  \n",
    "Question: \"Of all 'sick' predictions, how many were right?\"  \n",
    "Use when: False alarms are costly (spam filter, fraud detection)\n",
    "\n",
    "**Recall** = TP / (TP + FN)  \n",
    "Question: \"Of all actually sick, how many did we catch?\"  \n",
    "Use when: Missing positives is dangerous (cancer, disease detection)\n",
    "\n",
    "**F1-Score** = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)  \n",
    "The harmonic mean - balances both. Good single metric for imbalanced data.\n",
    "\n",
    "### The Tradeoff\n",
    "\n",
    "You can't maximize both precision and recall. It's a seesaw:\n",
    "\n",
    "- **High precision, low recall**: Strict model - only flags cases it's very sure about. Misses some sick patients but rarely gives false alarms.\n",
    "\n",
    "- **Low precision, high recall**: Sensitive model - flags anything suspicious. Catches all sick patients but triggers many false alarms.\n",
    "\n",
    "The right choice depends on the cost of each error type. For medical diagnosis, missing a sick patient (FN) is worse than a false alarm (FP), so we prioritize recall.\n",
    "\n",
    "Next: Cross-validation - getting reliable performance estimates instead of getting lucky (or unlucky) with a single train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cross-Validation - Testing Your Exam Five Times\n",
    "\n",
    "Imagine you're studying for an exam by practicing 100 questions. Your teacher randomly picks 25 questions for the actual test.\n",
    "\n",
    "**First try:** The 25 test questions happen to cover your strongest topics. You score 95%!  \n",
    "**Second try:** Different 25 questions, but they hit your weak spots. You score 65%.  \n",
    "**Your actual knowledge?** Probably somewhere around 80%.\n",
    "\n",
    "Here's the problem: a single train/test split is like taking that exam once with random questions. Your score might be lucky or unlucky. You won't really know how good you are until you test yourself multiple times with different question sets.\n",
    "\n",
    "This is exactly what cross-validation does for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 The Multiple Test Strategy\n",
    "\n",
    "Instead of splitting your data once, cross-validation splits it multiple times and averages the results. It's like taking five different versions of the exam and calculating your average score.\n",
    "\n",
    "Here's how **5-fold cross-validation** works:\n",
    "\n",
    "Split your data into 5 equal parts (called folds). Then train and test 5 times, each time using a different fold as the test set:\n",
    "\n",
    "- Round 1: Train on folds 1,2,3,4 â†’ Test on fold 5\n",
    "- Round 2: Train on folds 1,2,3,5 â†’ Test on fold 4  \n",
    "- Round 3: Train on folds 1,2,4,5 â†’ Test on fold 3\n",
    "- Round 4: Train on folds 2,3,4,5 â†’ Test on fold 1\n",
    "- Round 5: Train on folds 1,3,4,5 â†’ Test on fold 2\n",
    "\n",
    "Your final score is the average of these 5 test scores, plus the standard deviation to show how much variance there is.\n",
    "\n",
    "Why this matters:\n",
    "- Every single data point gets used for testing exactly once\n",
    "- Every data point gets used for training 4 times\n",
    "- You get a more reliable estimate (not just lucky or unlucky)\n",
    "- You get both the mean and the variance (confidence in your score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import time\n",
    "\n",
    "print(\"Cross-validation tools loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Running Cross-Validation\n",
    "\n",
    "Let's evaluate our disease classifier using 5-fold stratified cross-validation with three metrics: accuracy (to see the paradox again), F1-score (balanced metric), and recall (catching sick patients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Keeping the Balance\n",
    "\n",
    "There's one problem with regular k-fold cross-validation on imbalanced data. With 95% healthy and 5% sick patients, a random fold might end up with:\n",
    "\n",
    "- Fold 1: 98% healthy, 2% sick\n",
    "- Fold 2: 92% healthy, 8% sick  \n",
    "- Fold 3: 96% healthy, 4% sick\n",
    "\n",
    "This variability makes results unreliable. **Stratified k-fold** solves this by maintaining the exact same class ratio in every fold.\n",
    "\n",
    "$$\\text{Class Ratio}_{\\text{each fold}} = \\text{Class Ratio}_{\\text{original}}$$\n",
    "\n",
    "In plain English: if your original data is 95% healthy and 5% sick, then every single fold will be 95% healthy and 5% sick. No exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Setup: 5-fold cross-validation (stratified)\")\n",
    "print(f\"Each fold: ~{len(y)//5} samples\")\n",
    "print(f\"Process: Train 5 models, each using 4/5 of data for training\")\n",
    "print(f\"Result: Average performance across all 5 test sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cv = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "print(\"Running 5-fold cross-validation with 3 metrics...\")\n",
    "start_time = time.time()\n",
    "\n",
    "cv_scores_accuracy = cross_val_score(clf_cv, X, y, cv=skf, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(clf_cv, X, y, cv=skf, scoring='f1')\n",
    "cv_scores_recall = cross_val_score(clf_cv, X, y, cv=skf, scoring='recall')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Complete! Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Comparing Results Across Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CROSS-VALIDATION RESULTS (5 Folds)\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nACCURACY (misleading for imbalanced data):\")\n",
    "for i, score in enumerate(cv_scores_accuracy, 1):\n",
    "    print(f\"   Fold {i}: {score:.4f} ({score*100:.2f}%)\")\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   Mean:   {cv_scores_accuracy.mean():.4f} ({cv_scores_accuracy.mean()*100:.2f}%)\")\n",
    "print(f\"   Std:    {cv_scores_accuracy.std():.4f}\")\n",
    "\n",
    "print(\"\\nF1-SCORE (balanced metric):\")\n",
    "for i, score in enumerate(cv_scores_f1, 1):\n",
    "    print(f\"   Fold {i}: {score:.4f} ({score*100:.2f}%)\")\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   Mean:   {cv_scores_f1.mean():.4f} ({cv_scores_f1.mean()*100:.2f}%)\")\n",
    "print(f\"   Std:    {cv_scores_f1.std():.4f}\")\n",
    "\n",
    "print(\"\\nRECALL (catching sick patients):\")\n",
    "for i, score in enumerate(cv_scores_recall, 1):\n",
    "    print(f\"   Fold {i}: {score:.4f} ({score*100:.2f}%)\")\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   Mean:   {cv_scores_recall.mean():.4f} ({cv_scores_recall.mean()*100:.2f}%)\")\n",
    "print(f\"   Std:    {cv_scores_recall.std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What This Tells Us\n",
    "\n",
    "Notice how accuracy looks great (around 95%), but recall is lower. That's the accuracy paradox again - the model is still missing sick patients.\n",
    "\n",
    "The standard deviation tells you about stability. A low standard deviation (like 0.01) means the model performs consistently across different data splits. A high standard deviation (like 0.10) means the model's performance varies a lot depending on which data it sees - that's a red flag.\n",
    "\n",
    "Cross-validation gives you confidence: instead of one score that might be lucky, you have five scores that show the real pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_data = [\n",
    "    ('Accuracy', cv_scores_accuracy, '#3498db'),\n",
    "    ('F1-Score', cv_scores_f1, '#f39c12'),\n",
    "    ('Recall', cv_scores_recall, '#2ecc71')\n",
    "]\n",
    "\n",
    "for idx, (name, scores, color) in enumerate(metrics_data):\n",
    "    ax = axes[idx]\n",
    "    folds = np.arange(1, len(scores) + 1)\n",
    "    \n",
    "    bars = ax.bar(folds, scores, color=color, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    mean_line = ax.axhline(scores.mean(), color='red', linestyle='--', linewidth=2.5, \n",
    "                           label=f'Mean: {scores.mean():.4f}', zorder=10)\n",
    "    \n",
    "    ax.axhspan(scores.mean() - scores.std(), scores.mean() + scores.std(), \n",
    "               alpha=0.2, color='gray', label=f'Â±1 Std: {scores.std():.4f}')\n",
    "    \n",
    "    for bar, score in zip(bars, scores):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{score:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Fold Number', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(name, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{name} Across 5 Folds', fontsize=13, fontweight='bold')\n",
    "    ax.set_xticks(folds)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend(fontsize=10, loc='lower right')\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle=':', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The red dashed line shows the mean performance\")\n",
    "print(\"The gray band shows Â±1 standard deviation\")\n",
    "print(f\"Narrow band = stable model, wide band = unstable model\")\n",
    "print(f\"Our recall std: {cv_scores_recall.std():.4f} (stable!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 How Many Folds Should You Use?\n",
    "\n",
    "You might wonder: why 5 folds? Why not 10, or 20?\n",
    "\n",
    "There's a tradeoff. More folds means:\n",
    "- More accurate estimate (each training set uses more data)\n",
    "- Slower computation (you train more models)\n",
    "\n",
    "Let's compare k=5 vs k=10 to see if the extra work is worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Running 10-fold cross-validation...\")\n",
    "start_time_10 = time.time()\n",
    "\n",
    "cv_scores_f1_10 = cross_val_score(clf_cv, X, y, cv=skf_10, scoring='f1')\n",
    "cv_scores_recall_10 = cross_val_score(clf_cv, X, y, cv=skf_10, scoring='recall')\n",
    "\n",
    "end_time_10 = time.time()\n",
    "print(f\"Complete! Time: {end_time_10 - start_time_10:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"K-FOLD COMPARISON: k=5 vs k=10\".center(70))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nF1-SCORE:\")\n",
    "print(f\"   k=5:  Mean = {cv_scores_f1.mean():.4f}, Std = {cv_scores_f1.std():.4f}\")\n",
    "print(f\"   k=10: Mean = {cv_scores_f1_10.mean():.4f}, Std = {cv_scores_f1_10.std():.4f}\")\n",
    "print(f\"   Difference: {abs(cv_scores_f1.mean() - cv_scores_f1_10.mean()):.4f}\")\n",
    "\n",
    "print(\"\\nRECALL:\")\n",
    "print(f\"   k=5:  Mean = {cv_scores_recall.mean():.4f}, Std = {cv_scores_recall.std():.4f}\")\n",
    "print(f\"   k=10: Mean = {cv_scores_recall_10.mean():.4f}, Std = {cv_scores_recall_10.std():.4f}\")\n",
    "print(f\"   Difference: {abs(cv_scores_recall.mean() - cv_scores_recall_10.mean()):.4f}\")\n",
    "\n",
    "print(\"\\nCOMPUTATIONAL COST:\")\n",
    "print(f\"   k=5:  {end_time - start_time:.2f} seconds\")\n",
    "print(f\"   k=10: {end_time_10 - start_time_10:.2f} seconds\")\n",
    "print(f\"   Slowdown: {(end_time_10 - start_time_10)/(end_time - start_time):.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nConclusion: k=5 and k=10 give very similar results\")\n",
    "print(\"k=10 is slightly more stable but takes ~2x longer\")\n",
    "print(\"For most projects, k=5 is the sweet spot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "comparison_data = [cv_scores_recall, cv_scores_recall_10]\n",
    "\n",
    "bp = axes[0].boxplot(comparison_data, labels=['k=5 (5 folds)', 'k=10 (10 folds)'],\n",
    "                     patch_artist=True, widths=0.6)\n",
    "\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "axes[0].set_ylabel('Recall Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Recall Distribution: k=5 vs k=10', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "\n",
    "means = [cv_scores_recall.mean(), cv_scores_recall_10.mean()]\n",
    "axes[0].plot([1, 2], means, 'D', color='gold', markersize=10, label='Mean', zorder=10)\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(range(1, 6), cv_scores_recall, 'o-', color='#3498db', linewidth=2.5, \n",
    "             markersize=8, label=f'k=5 (Mean: {cv_scores_recall.mean():.4f})', alpha=0.8)\n",
    "axes[1].plot(range(1, 11), cv_scores_recall_10, 's-', color='#e74c3c', linewidth=2.5, \n",
    "             markersize=8, label=f'k=10 (Mean: {cv_scores_recall_10.mean():.4f})', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Fold Number', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Recall Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Fold-by-Fold Recall Scores', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Box plot shows the distribution of scores\")\n",
    "print(\"Line plot shows how each fold performed\")\n",
    "print(\"Both show k=10 has more data points but similar variation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Part 5 Summary\n",
    "\n",
    "Cross-validation solves the \"lucky split\" problem. Instead of trusting one random train/test split, you test your model multiple times with different splits and average the results.\n",
    "\n",
    "**The Core Idea:**\n",
    "\n",
    "With k-fold cross-validation, you split your data into k equal parts. Then you train k models, each time using k-1 parts for training and 1 part for testing. Every data point gets tested exactly once and trained on k-1 times.\n",
    "\n",
    "$$\\text{Final Score} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{Score}_i \\pm \\text{Standard Deviation}$$\n",
    "\n",
    "**Why Stratified Matters:**\n",
    "\n",
    "For imbalanced data (like 95% healthy, 5% sick), regular k-fold might create unbalanced folds. Stratified k-fold maintains the same class ratio in every fold, making results more reliable.\n",
    "\n",
    "**Choosing K:**\n",
    "\n",
    "- **k=3**: Fast but less reliable. Good for quick experiments.\n",
    "- **k=5**: The sweet spot. Fast enough and reliable enough for most projects.\n",
    "- **k=10**: More accurate but takes 2x longer. Use for final validation.\n",
    "- **k=20+**: Very slow. Only for small datasets or when you need maximum precision.\n",
    "\n",
    "The key insight from cross-validation is the standard deviation. A low standard deviation means your model is stable across different data splits. A high standard deviation means your model's performance varies a lot - that's a warning sign.\n",
    "\n",
    "**When to Use It:**\n",
    "\n",
    "Cross-validation is essential when you have limited data and need to squeeze out every bit of training and testing value. It's standard practice for model evaluation in most machine learning projects.\n",
    "\n",
    "**When to Skip It:**\n",
    "\n",
    "If you have millions of samples, a single train/test split is usually fine. Also skip it for time series data (use time-based splits instead) or when training is extremely expensive (like large deep learning models).\n",
    "\n",
    "The bottom line: cross-validation gives you confidence in your model's performance. Instead of one score that might be lucky, you get multiple scores that reveal the true pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
